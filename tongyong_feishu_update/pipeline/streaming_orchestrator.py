"""
æµå¼æ›´æ–°ç¼–æ’å™¨ - ä¿®å¤æ‰¹é‡å¤„ç†ç¼ºé™·

è§£å†³é—®é¢˜ï¼š
1. æ”¹ä¸ºæµå¼å¤„ç†ï¼šä¸€ä¸ªäº§å“å¤„ç†å®Œç«‹å³åŒæ­¥
2. æ·»åŠ è¿›åº¦ä¿å­˜ï¼šè®°å½•å·²å¤„ç†çš„äº§å“ID
3. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²å¤„ç†çš„äº§å“
4. å¢åŠ è¶…æ—¶æ§åˆ¶ï¼šå•ä¸ªäº§å“è¶…æ—¶ä¸å½±å“æ•´ä½“
"""

import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set
from ..models.product import Product
from ..models.update_result import UpdateResult
from ..models.progress import ProgressEvent
from ..services.field_assembler import FieldAssembler
from ..services.title_generator import TitleGenerator
from ..services.translator import Translator
from ..services.detail_fetcher import DetailFetcher
from ..clients.interfaces import GLMClientInterface, FeishuClientInterface
from ..loaders.factory import LoaderFactory


class StreamingUpdateOrchestrator:
    """æµå¼æ›´æ–°ç¼–æ’å™¨
    
    ç‰¹æ€§ï¼š
    - é€ä¸ªäº§å“æµå¼å¤„ç†ï¼Œç«‹å³åŒæ­¥åˆ°é£ä¹¦
    - è¿›åº¦è‡ªåŠ¨ä¿å­˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    - å•ä¸ªäº§å“å¤±è´¥ä¸å½±å“å…¶ä»–äº§å“
    - å®æ—¶è¿›åº¦åé¦ˆ
    """

    def __init__(
        self,
        glm_client: GLMClientInterface,
        feishu_client: FeishuClientInterface,
        title_generator: Optional[TitleGenerator] = None,
        translator: Optional[Translator] = None,
        field_assembler: Optional[FieldAssembler] = None,
        progress_callback: Optional[callable] = None,
        progress_save_interval: int = 5,  # æ¯å¤„ç†5ä¸ªäº§å“ä¿å­˜ä¸€æ¬¡è¿›åº¦
        single_timeout: int = 60,  # å•ä¸ªäº§å“å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    ) -> None:
        self.glm_client = glm_client
        self.feishu_client = feishu_client
        self.title_generator = title_generator or TitleGenerator(glm_client)
        self.translator = translator or Translator(glm_client)
        self.field_assembler = field_assembler or FieldAssembler(
            title_generator=self.title_generator,
            translator=self.translator,
        )
        self.detail_fetcher = DetailFetcher()
        self.progress_callback = progress_callback
        self.progress_save_interval = progress_save_interval
        self.single_timeout = single_timeout

    def execute(
        self,
        input_path: str,
        *,
        force_update: bool = False,
        title_only: bool = False,
        category_only: bool = False,
        dry_run: bool = False,
        resume: bool = True,  # æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
    ) -> UpdateResult:
        """æ‰§è¡Œæµå¼æ›´æ–°æµç¨‹"""
        
        # åˆ›å»ºè¿›åº¦æ–‡ä»¶è·¯å¾„
        progress_file = self._get_progress_file_path(input_path)
        
        # 1. è¯»å–å¹¶è§£æäº§å“æ•°æ®
        data = json.load(open(input_path, 'r', encoding='utf-8'))
        loader = LoaderFactory.create(data)
        product_list = loader.parse(data)
        products = {p.product_id: p for p in product_list if p.product_id}

        # progress callback é€šçŸ¥åŠ è½½å®Œæˆ
        if self.progress_callback:
            event = ProgressEvent.progress_update_event(
                processed_count=0,
                total_count=len(products),
                success_count=0,
                failed_count=0
            )
            event.message = f"å·²åŠ è½½ {len(products)} ä¸ªäº§å“ï¼Œå‡†å¤‡æµå¼å¤„ç†"
            self.progress_callback(event)

        # 2. è·å–é£ä¹¦ç°æœ‰è®°å½•
        print("ğŸ” è·å–é£ä¹¦ç°æœ‰è®°å½•...")
        existing_records = self.feishu_client.get_records()
        existing_records_by_url = self.feishu_client.get_records_by_url()

        # 3. ç¡®ä¿è®°å½•å­˜åœ¨
        # 3.1 å…ˆå°è¯•é€šè¿‡URLæˆ– legacy ID åŒ¹é…
        missing_ids = []
        for pid in products.keys():
            if pid not in existing_records:
                product = products[pid]

                extra_data = getattr(product, 'extra', {}) or {}
                legacy_id = (
                    getattr(product, 'legacy_product_id', '') or
                    getattr(product, 'legacyProductId', '') or
                    (extra_data.get('legacyProductId') if isinstance(extra_data, dict) else '')
                )
                legacy_id = str(legacy_id).strip()
                if legacy_id and legacy_id in existing_records:
                    existing_records[pid] = existing_records[legacy_id]
                    print(f"ğŸ” ä½¿ç”¨ legacyProductId åŒ¹é…åˆ°ç°æœ‰è®°å½•: {pid} <- {legacy_id}")
                    continue

                detail_url = (getattr(product, 'detail_url', None) or
                             getattr(product, 'detailUrl', None) or '').strip().rstrip('/')
                if detail_url and detail_url in existing_records_by_url:
                    existing_records[pid] = existing_records_by_url[detail_url]
                    print(f"ğŸ” ä½¿ç”¨å•†å“é“¾æ¥åŒ¹é…åˆ°ç°æœ‰è®°å½•: {pid} -> {detail_url}")
                else:
                    missing_ids.append(pid)
        if missing_ids:
            print(f"å‘ç° {len(missing_ids)} ä¸ªç¼ºå¤±çš„product_idï¼Œæ­£åœ¨æ‰¹é‡åˆ›å»º...")
            self._create_missing_records(missing_ids, products, dry_run)
            # é‡æ–°è·å–é£ä¹¦è®°å½•
            existing_records = self.feishu_client.get_records()

        # 4. è®¡ç®—éœ€è¦å¤„ç†çš„äº§å“
        fields_to_check = self._get_fields_to_check(title_only, category_only)
        candidate_ids, skipped_ids = self._calculate_candidates(
            products, existing_records, fields_to_check, force_update
        )

        if not candidate_ids:
            print("âœ… æ²¡æœ‰éœ€è¦æ›´æ–°çš„äº§å“")
            return UpdateResult(
                success_count=0,
                failed_batches=[],
                candidates_count=0,
                skipped_count=len(skipped_ids),
                title_failed=[],
                total_batches=0,
                log_path=None
            )

        # 5. åŠ è½½è¿›åº¦ï¼ˆå¦‚æœå¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼‰
        processed_ids = set()
        if resume:
            processed_ids = self._load_progress(progress_file)
            remaining_candidates = [pid for pid in candidate_ids if pid not in processed_ids]
            if processed_ids:
                print(f"ğŸ“ æ–­ç‚¹ç»­ä¼ ï¼šå·²å¤„ç† {len(processed_ids)} ä¸ªäº§å“ï¼Œå‰©ä½™ {len(remaining_candidates)} ä¸ª")
                candidate_ids = remaining_candidates

        print(f"ğŸš€ å¼€å§‹æµå¼å¤„ç† {len(candidate_ids)} ä¸ªäº§å“...")

        # 6. æµå¼å¤„ç†æ¯ä¸ªäº§å“
        return self._process_products_streaming(
            candidate_ids,
            products,
            existing_records,
            title_only,
            category_only,
            force_update,
            dry_run,
            progress_file,
            processed_ids,
            len(skipped_ids)
        )

    def _process_products_streaming(
        self,
        candidate_ids: List[str],
        products: Dict[str, Product],
        existing_records: Dict[str, Dict],
        title_only: bool,
        category_only: bool,
        force_update: bool,
        dry_run: bool,
        progress_file: Path,
        initial_processed: Set[str],
        skipped_count: int
    ) -> UpdateResult:
        """æµå¼å¤„ç†äº§å“åˆ—è¡¨"""
        
        success_count = 0
        failed_products = []
        processed_count = len(initial_processed)
        total_count = len(candidate_ids) + processed_count

        for i, product_id in enumerate(candidate_ids, 1):
            try:
                print(f"\nğŸ“¦ å¤„ç†äº§å“ {i}/{len(candidate_ids)}: {product_id}")
                
                # å•ä¸ªäº§å“å¤„ç†è¶…æ—¶æ§åˆ¶
                start_time = time.time()
                
                success = self._process_single_product(
                    product_id,
                    products[product_id],
                    existing_records[product_id],
                    title_only,
                    category_only,
                    force_update,
                    dry_run
                )
                
                elapsed = time.time() - start_time
                
                if success:
                    success_count += 1
                    print(f"  âœ… æˆåŠŸ (è€—æ—¶: {elapsed:.1f}s)")
                else:
                    failed_products.append(product_id)
                    print(f"  âŒ å¤±è´¥ (è€—æ—¶: {elapsed:.1f}s)")
                
                processed_count += 1
                
                # æ›´æ–°è¿›åº¦å›è°ƒ
                if self.progress_callback:
                    event = ProgressEvent.progress_update_event(
                        processed_count=processed_count,
                        total_count=total_count,
                        success_count=success_count,
                        failed_count=len(failed_products)
                    )
                    event.message = f"æµå¼å¤„ç†è¿›åº¦: {processed_count}/{total_count}"
                    self.progress_callback(event)
                
                # å®šæœŸä¿å­˜è¿›åº¦
                if i % self.progress_save_interval == 0:
                    processed_ids = initial_processed.union(set(candidate_ids[:i]))
                    self._save_progress(progress_file, processed_ids)
                    print(f"  ğŸ’¾ è¿›åº¦å·²ä¿å­˜ ({len(processed_ids)} ä¸ªäº§å“)")

            except Exception as e:
                print(f"  ğŸ’¥ å¤„ç†äº§å“ {product_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                failed_products.append(product_id)
                processed_count += 1

        # æœ€ç»ˆä¿å­˜è¿›åº¦
        final_processed = initial_processed.union(set(candidate_ids))
        self._save_progress(progress_file, final_processed)
        
        print(f"\nğŸ‰ æµå¼å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {len(failed_products)}")

        return UpdateResult(
            success_count=success_count,
            failed_batches=[{'products': failed_products}] if failed_products else [],
            candidates_count=len(candidate_ids),
            skipped_count=skipped_count,
            title_failed=failed_products,
            total_batches=1,
            log_path=str(progress_file)
        )

    def _process_single_product(
        self,
        product_id: str,
        product: Product,
        record_info: Dict,
        title_only: bool,
        category_only: bool,
        force_update: bool,
        dry_run: bool
    ) -> bool:
        """å¤„ç†å•ä¸ªäº§å“ï¼šç”Ÿæˆæ ‡é¢˜ â†’ æŠ“å–è¯¦æƒ…ï¼ˆå¦‚éœ€è¦ï¼‰ â†’ ç»„è£…å­—æ®µ â†’ ç«‹å³åŒæ­¥"""

        try:
            # 1. ç”Ÿæˆæ ‡é¢˜ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
            print(f"  ğŸ·ï¸ ç”Ÿæˆæ ‡é¢˜...")
            title = self._generate_title_with_timeout(product)

            # 2. è½¬æ¢Productå¯¹è±¡ä¸ºå­—å…¸æ ¼å¼ï¼Œé€ä¼  extra æ•°æ®
            if hasattr(product, '__dict__'):
                # è·å– extra å­—æ®µä¸­çš„æ•°æ®
                extra = getattr(product, 'extra', {})

                product_dict = {
                    'productId': product.product_id,
                    'detailUrl': product.detail_url,
                    'colors': getattr(product, 'colors', []),
                    'sizes': getattr(product, 'sizes', []),
                    'imageUrls': getattr(product, 'imageUrls', []),  # æ–°å¢ï¼šå›¾ç‰‡URLåˆ—è¡¨
                    'sizeChart': getattr(product, 'sizeChart', {}),  # æ–°å¢ï¼šå°ºç è¡¨
                    'imagesMetadata': getattr(product, 'images_metadata', []),
                    'productName': getattr(product, 'product_name', ''),
                    'brand': getattr(product, 'brand', ''),
                    'priceText': getattr(product, 'price', ''),
                    'currentPrice': getattr(product, 'current_price', ''),
                    'description': getattr(product, 'description', ''),
                    'gender': getattr(product, 'gender', ''),  # ğŸ”¥ æ·»åŠ æ€§åˆ«å­—æ®µ
                    'category': getattr(product, 'category', ''),  # ğŸ”¥ æ·»åŠ åˆ†ç±»å­—æ®µ
                    # ä» extra ä¸­é€ä¼  _detail_data å’Œå…¶ä»–åŸå§‹æ•°æ®
                    **extra
                }
            else:
                product_dict = product

            # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦æŠ“å–è¯¦æƒ…
            if self.detail_fetcher.needs_detail_fetch(product_dict):
                print(f"  ğŸ“„ äº§å“ {product_id} éœ€è¦è¡¥å……è¯¦æƒ…æ•°æ®...")
                detail_data = self.detail_fetcher.fetch_product_detail(
                    product_dict.get('detailUrl', ''),
                    product_id
                )
                if detail_data:
                    # åˆå¹¶è¯¦æƒ…æ•°æ®åˆ°äº§å“å­—å…¸
                    enhanced_dict = self.detail_fetcher.merge_detail_into_product(product_dict, detail_data)
                    product_dict = enhanced_dict
                    print(f"  âœ… è¯¦æƒ…æŠ“å–æˆåŠŸ")
                else:
                    print(f"  âš ï¸ è¯¦æƒ…æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®")

            # 4. æå–è¯¦æƒ…æ•°æ®
            detail_data = product_dict.get('_detail_data')

            # 5. ç»„è£…å­—æ®µ
            fields = self.field_assembler.build_update_fields(
                product=product_dict,
                pre_generated_title=title,
                title_only=title_only,
                category_only=category_only,
                product_detail=detail_data
            )

            if not fields:
                print(f"  âš ï¸ æ²¡æœ‰å­—æ®µéœ€è¦æ›´æ–°")
                return True

            # è°ƒè¯•ï¼šæ‰“å°ç”Ÿæˆçš„å­—æ®µ
            print(f"\nğŸ“‹ äº§å“ {product_id} ç”Ÿæˆçš„å­—æ®µ:")
            print("=" * 60)
            for field_name, field_value in fields.items():
                if field_name in ['é¢œè‰²', 'å°ºç ', 'å›¾ç‰‡URL', 'å°ºç è¡¨', 'è¯¦æƒ…é¡µæ–‡å­—']:
                    # å¤šè¡Œå­—æ®µæ˜¾ç¤ºè¡Œæ•°
                    if isinstance(field_value, str):
                        lines = field_value.split('\n')
                        print(f"  {field_name}: {len(lines)}è¡Œ (æ€»é•¿åº¦:{len(field_value)}å­—ç¬¦)")
                    else:
                        print(f"  {field_name}: {field_value}")
                else:
                    print(f"  {field_name}: {field_value}")
            print("=" * 60)
            print(f"å…±ç”Ÿæˆ {len(fields)} ä¸ªå­—æ®µ\n")
                
            # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if not force_update and not self._fields_are_different(record_info['fields'], fields):
                print(f"  â­ï¸ å­—æ®µæ— å˜åŒ–ï¼Œè·³è¿‡æ›´æ–°")
                return True
            
            # 4. ç«‹å³åŒæ­¥åˆ°é£ä¹¦
            if dry_run:
                print(f"  ğŸ” æ¨¡æ‹Ÿæ¨¡å¼ï¼šè·³è¿‡é£ä¹¦æ›´æ–°")
                return True
                
            print(f"  â¬†ï¸ åŒæ­¥åˆ°é£ä¹¦...")
            update_record = {
                'record_id': record_info['record_id'],
                'fields': fields
            }
            
            # å•ä¸ªè®°å½•æ›´æ–°
            result = self.feishu_client.batch_update([update_record], batch_size=1)
            return result.get('success_count', 0) > 0
            
        except Exception as e:
            print(f"  ğŸ’¥ å¤„ç†å¤±è´¥: {e}")
            return False

    def _generate_title_with_timeout(self, product: Product) -> str:
        """å¸¦è¶…æ—¶æ§åˆ¶çš„æ ‡é¢˜ç”Ÿæˆ"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´ç²¾ç»†çš„è¶…æ—¶æ§åˆ¶
            title = self.title_generator.generate(product)
            return title or ""
        except Exception as e:
            print(f"    âš ï¸ æ ‡é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def _get_progress_file_path(self, input_path: str) -> Path:
        """è·å–è¿›åº¦æ–‡ä»¶è·¯å¾„"""
        input_file = Path(input_path)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        progress_file = input_file.parent / f"streaming_progress_{input_file.stem}_{timestamp}.json"
        return progress_file

    def _save_progress(self, progress_file: Path, processed_ids: Set[str]) -> None:
        """ä¿å­˜å¤„ç†è¿›åº¦"""
        try:
            progress_data = {
                'timestamp': datetime.now().isoformat(),
                'processed_count': len(processed_ids),
                'processed_ids': list(processed_ids)
            }
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

    def _load_progress(self, progress_file: Path) -> Set[str]:
        """åŠ è½½å¤„ç†è¿›åº¦"""
        try:
            if not progress_file.exists():
                return set()
                
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
                processed_ids = set(progress_data.get('processed_ids', []))
                return processed_ids
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            return set()

    def _create_missing_records(self, missing_ids: List[str], products: Dict[str, Product], dry_run: bool):
        """åˆ›å»ºç¼ºå¤±çš„è®°å½•ï¼ˆå¸¦IDå»é‡ä¿æŠ¤ï¼‰"""
        # è·å–å·²å­˜åœ¨çš„å•†å“IDé›†åˆ
        existing_ids = self.feishu_client.get_existing_ids()

        create_records = []
        skipped_ids = []

        for pid in missing_ids:
            # æ£€æŸ¥IDæ˜¯å¦å·²å­˜åœ¨ï¼ˆé˜²æ­¢é‡å¤åˆ›å»ºï¼‰
            if pid in existing_ids:
                print(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨çš„å•†å“ID: {pid}ï¼ˆURLä¸åŒ¹é…ä½†IDå·²å­˜åœ¨ï¼‰")
                skipped_ids.append(pid)
                continue

            product = products[pid]
            create_records.append({
                'fields': {
                    'å•†å“ID': pid,
                    'å•†å“é“¾æ¥': product.detail_url or '',
                    'å“ç‰Œå': product.brand or '',
                },
                'product_id': pid
            })

        if skipped_ids:
            print(f"âš ï¸ è·³è¿‡ {len(skipped_ids)} ä¸ªå·²å­˜åœ¨çš„å•†å“ID")

        if not create_records:
            print("âœ… æ²¡æœ‰éœ€è¦åˆ›å»ºçš„æ–°è®°å½•")
            return

        if not dry_run:
            create_result = self.feishu_client.batch_create(create_records, batch_size=30)
            if create_result.get('success_count', 0) > 0:
                print(f"âœ… æˆåŠŸåˆ›å»º {create_result['success_count']} æ¡æ–°è®°å½•")
                # å°†æ–°åˆ›å»ºçš„IDæ·»åŠ åˆ°existing_idsé›†åˆ
                for record in create_records:
                    existing_ids.add(record['product_id'])
            else:
                if not skipped_ids:  # åªæœ‰åœ¨æ²¡æœ‰è·³è¿‡ä»»ä½•è®°å½•æ—¶æ‰æŠ¥é”™
                    raise RuntimeError("ç¼ºå¤±è®°å½•åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
        else:
            print(f"æ¨¡æ‹Ÿæ¨¡å¼ï¼šè·³è¿‡åˆ›å»º {len(create_records)} æ¡ç¼ºå¤±è®°å½•")

    def _get_fields_to_check(self, title_only: bool, category_only: bool) -> List[str]:
        """è·å–éœ€è¦æ£€æŸ¥çš„å­—æ®µåˆ—è¡¨"""
        fields_to_check = ['å•†å“ID','å•†å“æ ‡é¢˜','ä»·æ ¼','æ€§åˆ«','è¡£æœåˆ†ç±»','å“ç‰Œå',
                          'é¢œè‰²','å°ºç ','å›¾ç‰‡URL','å›¾ç‰‡æ•°é‡','è¯¦æƒ…é¡µæ–‡å­—','å•†å“é“¾æ¥','å°ºç è¡¨']
        if title_only:
            fields_to_check = ['å•†å“æ ‡é¢˜']
        elif category_only:
            fields_to_check = ['è¡£æœåˆ†ç±»']
        return fields_to_check

    def _calculate_candidates(
        self,
        products: Dict[str, Product],
        existing_records: Dict[str, Dict],
        fields_to_check: List[str],
        force_update: bool
    ):
        """è®¡ç®—éœ€è¦å¤„ç†çš„å€™é€‰äº§å“"""
        candidate_ids = []
        skipped_ids = []
        
        for product_id, product in products.items():
            if product_id in existing_records:
                existing_fields = existing_records[product_id]['fields']
                
                if force_update:
                    candidate_ids.append(product_id)
                else:
                    empty_fields = self._has_empty_fields_to_fill(existing_fields, fields_to_check)
                    if empty_fields:
                        candidate_ids.append(product_id)
                    else:
                        skipped_ids.append(product_id)
            else:
                print(f"è­¦å‘Šï¼šäº§å“ {product_id} åœ¨ç¼ºå¤±è®°å½•åˆ›å»ºåä»ä¸å­˜åœ¨äº existing_records ä¸­")
                candidate_ids.append(product_id)
                
        return candidate_ids, skipped_ids

    def _fields_are_different(self, existing_fields: Dict, new_fields: Dict) -> bool:
        """æ£€æŸ¥å­—æ®µæ˜¯å¦æœ‰å˜åŒ–"""
        for key, new_value in new_fields.items():
            existing_value = existing_fields.get(key, '')
            if self._normalize_field_value(new_value) != self._normalize_field_value(existing_value):
                return True
        return False

    def _has_empty_fields_to_fill(self, existing_fields: Dict, target_fields: List[str]) -> List[str]:
        """æ£€æŸ¥æŒ‡å®šå­—æ®µä¸­å“ªäº›ä¸ºç©ºéœ€è¦è¡¥é½"""
        empty_fields = []
        for field in target_fields:
            existing_value = self._normalize_field_value(existing_fields.get(field, ''))
            if not existing_value:
                empty_fields.append(field)
        return empty_fields

    def _normalize_field_value(self, value: any) -> str:
        """å°†é£ä¹¦å­—æ®µå€¼ç»Ÿä¸€è½¬æˆå¯æ¯”å¯¹çš„å­—ç¬¦ä¸²ï¼Œå…¼å®¹å•é€‰/å¤šé€‰ç»“æ„"""
        if isinstance(value, dict):
            # å•é€‰/å¤šé€‰å¯èƒ½è¿”å› {text, option_id}
            if 'text' in value:
                return str(value.get('text') or '').strip()
            return str(value).strip()
        if isinstance(value, list):
            # å¤šé€‰/å•é€‰æ•°ç»„ï¼šæå–æ–‡æœ¬åç”¨æ¢è¡Œæ‹¼æ¥ä¿æŒé¡ºåº
            normalized_items = []
            for item in value:
                if isinstance(item, dict) and 'text' in item:
                    normalized_items.append(str(item.get('text') or '').strip())
                else:
                    normalized_items.append(str(item).strip())
            return '\n'.join(filter(None, normalized_items))
        return str(value or '').strip()
