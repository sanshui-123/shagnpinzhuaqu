#!/usr/bin/env python3
"""
é£ä¹¦åŒæ­¥è„šæœ¬ - å¢é‡åŒæ­¥äº§å“æ•°æ®
è¯»å– all_products_dedup.json å¹¶ä»…åŒæ­¥ç¼ºå¤±çš„è®°å½•

æ–°å¢åŠŸèƒ½:
- é£ä¹¦IDç¼“å­˜æœºåˆ¶: ç¼“å­˜ç°æœ‰å•†å“IDï¼Œæœ‰æ•ˆæœŸ30åˆ†é’Ÿï¼Œé¿å…é‡å¤APIè°ƒç”¨
- ç¼“å­˜æ–‡ä»¶: CallawayJP/results/feishu_id_cache.json
- ç¼“å­˜å­—æ®µ: fetchedAt (ISOæ—¶é—´æˆ³), ids (å•†å“IDåˆ—è¡¨)

æ–°å¢å‚æ•°:
- --refresh-cache: å¼ºåˆ¶è·³è¿‡ç¼“å­˜é‡æ–°æ‹‰å–é£ä¹¦ID

ç¤ºä¾‹å‘½ä»¤:
python3 sync_feishu_products.py --input all_products_dedup_*.json
python3 sync_feishu_products.py --input all_products_dedup_*.json --refresh-cache  # å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
"""

import json
import math
import requests
from pathlib import Path
import time
import os
import sys
import argparse
from datetime import datetime
from typing import Dict, List, Set

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from feishu_update.config.settings import resolve_feishu_config_path

def load_config():
    """åŠ è½½é£ä¹¦é…ç½®"""
    config_path = resolve_feishu_config_path()
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config['feishu']

def get_token(app_id, app_secret):
    """è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ"""
    url = 'https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal'
    resp = requests.post(url, json={'app_id': app_id, 'app_secret': app_secret}, timeout=15)
    resp.raise_for_status()
    data = resp.json()
    if data.get('code') != 0:
        raise RuntimeError(f"failed to get token: {data}")
    return data['tenant_access_token']

def load_feishu_id_cache(cache_file: Path, cache_validity_minutes: int = 30) -> Set[str]:
    """
    åŠ è½½é£ä¹¦IDç¼“å­˜
    
    Args:
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_validity_minutes: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆåˆ†é’Ÿï¼‰
    
    Returns:
        Set[str]: ç¼“å­˜çš„å•†å“IDé›†åˆï¼Œå¦‚æœç¼“å­˜å¤±æ•ˆæˆ–ä¸å­˜åœ¨åˆ™è¿”å›ç©ºé›†åˆ
    """
    if not cache_file.exists():
        return set()
    
    try:
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…
        fetched_at_str = cache_data.get('fetchedAt', '')
        if fetched_at_str:
            fetched_at = datetime.fromisoformat(fetched_at_str.replace('Z', '+00:00'))
            now = datetime.now(fetched_at.tzinfo)
            age_minutes = (now - fetched_at).total_seconds() / 60
            
            if age_minutes <= cache_validity_minutes:
                ids = set(cache_data.get('ids', []))
                print(f"ğŸ”„ ç¼“å­˜å‘½ä¸­: åŠ è½½ {len(ids)} ä¸ªå•†å“ID (ç¼“å­˜æ—¶é—´: {age_minutes:.1f}åˆ†é’Ÿå‰)")
                return ids
            else:
                print(f"â° ç¼“å­˜å¤±æ•ˆ: ç¼“å­˜æ—¶é—´ {age_minutes:.1f}åˆ†é’Ÿ > æœ‰æ•ˆæœŸ {cache_validity_minutes}åˆ†é’Ÿ")
        
    except Exception as e:
        print(f"âš ï¸  ç¼“å­˜è¯»å–å¤±è´¥: {e}")
    
    return set()

def save_feishu_id_cache(cache_file: Path, ids: Set[str]):
    """
    ä¿å­˜é£ä¹¦IDç¼“å­˜
    
    Args:
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        ids: å•†å“IDé›†åˆ
    """
    try:
        cache_data = {
            'fetchedAt': datetime.now().isoformat() + 'Z',
            'ids': list(ids)
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç¼“å­˜å·²æ›´æ–°: {len(ids)} ä¸ªå•†å“ID")
        
    except Exception as e:
        print(f"âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

def get_existing_records(app_token: str, table_id: str, token: str) -> Set[str]:
    """è·å–é£ä¹¦è¡¨ä¸­ç°æœ‰çš„æ‰€æœ‰å•†å“ID"""
    url = f'https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records'
    headers = {
        'Authorization': f'Bearer {token}',
        'Content-Type': 'application/json'
    }
    
    existing_ids = set()
    page_token = None
    
    while True:
        params = {
            'page_size': 500,
            'field_names': '["å•†å“ID"]'
        }
        if page_token:
            params['page_token'] = page_token
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶
        for attempt in range(3):
            try:
                resp = requests.get(url, headers=headers, params=params, timeout=30)
                resp.raise_for_status()
                data = resp.json()
                
                if data.get('code') != 0:
                    break
                
                # æå–å•†å“ID
                items = data.get('data', {}).get('items', [])
                existing_ids.update(
                    item.get('fields', {}).get('å•†å“ID')
                    for item in items
                    if item.get('fields', {}).get('å•†å“ID')
                )
                
                page_token = data.get('data', {}).get('page_token')
                break
            except Exception as e:
                if attempt == 2:
                    raise e
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        if not page_token:
            break
        
        time.sleep(0.2)
    
    return existing_ids

def get_existing_records_with_cache(app_token: str, table_id: str, token: str, 
                                   cache_file: Path, refresh_cache: bool = False) -> Set[str]:
    """
    è·å–é£ä¹¦è¡¨ä¸­ç°æœ‰çš„æ‰€æœ‰å•†å“IDï¼ˆæ”¯æŒç¼“å­˜ï¼‰
    
    Args:
        app_token: é£ä¹¦åº”ç”¨token
        table_id: é£ä¹¦è¡¨ID
        token: è®¿é—®token
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        refresh_cache: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
    
    Returns:
        Set[str]: ç°æœ‰å•†å“IDé›†åˆ
    """
    if not refresh_cache:
        # å°è¯•åŠ è½½ç¼“å­˜
        cached_ids = load_feishu_id_cache(cache_file)
        if cached_ids:
            return cached_ids
    
    # ç¼“å­˜å¤±æ•ˆæˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œä»é£ä¹¦APIè·å–
    print("ğŸ”„ ä»é£ä¹¦APIè·å–å•†å“ID...")
    existing_ids = get_existing_records(app_token, table_id, token)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    save_feishu_id_cache(cache_file, existing_ids)
    
    return existing_ids

def batch_create_with_retry(app_token: str, table_id: str, token: str, records: List[Dict]) -> int:
    """å¸¦é‡è¯•æœºåˆ¶çš„æ‰¹é‡åˆ›å»º"""
    url = f'https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/batch_create'
    headers = {
        'Authorization': f'Bearer {token}',
        'Content-Type': 'application/json'
    }
    payload = {'records': records}
    
    for attempt in range(3):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            
            if data.get('code') != 0:
                raise RuntimeError(f"API error: {data}")
            
            return len(records)
        except Exception as e:
            if attempt == 2:
                raise e
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    return 0

def find_latest_dedup_file(results_dir: Path) -> Path:
    """æŸ¥æ‰¾æœ€æ–°çš„å»é‡æ–‡ä»¶"""
    # ä½¿ç”¨ Path.glob è·å–æ‰€æœ‰å»é‡æ–‡ä»¶
    dedup_files = list(results_dir.glob('all_products_dedup*.json'))
    
    if not dedup_files:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶ï¼Œæ£€æŸ¥é»˜è®¤æ–‡ä»¶
        old_file = results_dir / 'all_products_dedup.json'
        if old_file.exists():
            return old_file
        raise FileNotFoundError("æœªæ‰¾åˆ°å»é‡æ•°æ®æ–‡ä»¶")
    
    # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    return max(dedup_files, key=lambda f: f.stat().st_mtime)

def main():
    parser = argparse.ArgumentParser(description='å¢é‡åŒæ­¥ CallawayJP äº§å“åˆ°é£ä¹¦')
    parser.add_argument('--input', type=str, help='æŒ‡å®šè¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--refresh-cache', action='store_true', help='å¼ºåˆ¶è·³è¿‡ç¼“å­˜é‡æ–°æ‹‰å–é£ä¹¦å•†å“ID')
    args = parser.parse_args()
    
    # åŠ è½½å»é‡åçš„äº§å“æ•°æ®
    results_dir = Path(__file__).parent.parent / 'results'
    
    try:
        if args.input:
            dedup_file = Path(args.input)
        else:
            dedup_file = find_latest_dedup_file(results_dir)
        
        with open(dedup_file, 'r', encoding='utf-8') as f:
            products_data = json.load(f)
        
        all_products = products_data['products']
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # è·å–é£ä¹¦é…ç½®å’Œä»¤ç‰Œ
    try:
        feishu = load_config()
        token = get_token(feishu['app_id'], feishu['app_secret'])
    except Exception as e:
        print(f"âœ— è®¤è¯å¤±è´¥: {e}")
        return
    
    # è·å–é£ä¹¦ç°æœ‰è®°å½•ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
    cache_file = results_dir / 'feishu_id_cache.json'
    try:
        existing_ids = get_existing_records_with_cache(
            feishu['app_token'], feishu['table_id'], token, 
            cache_file, args.refresh_cache
        )
    except Exception as e:
        print(f"âœ— è·å–ç°æœ‰è®°å½•å¤±è´¥: {e}")
        return
    
    # ç­›é€‰å‡ºæ–°äº§å“
    new_products = [p for p in all_products if p['productId'] not in existing_ids]
    
    if not new_products:
        print("âœ“ æ— æ–°äº§å“ï¼Œè·³è¿‡åŒæ­¥")
        return
    
    # æ‰¹é‡åŒæ­¥æ–°äº§å“
    batch_size = 30
    success_count = 0
    failed_batches = []
    
    total_batches = math.ceil(len(new_products) / batch_size)
    
    for i in range(total_batches):
        chunk = new_products[i * batch_size:(i + 1) * batch_size]
        records = [
            {
                'fields': {
                    'å•†å“ID': item['productId'],
                    'å•†å“é“¾æ¥': item['detailUrl']
                }
            }
            for item in chunk
        ]
        
        try:
            batch_success = batch_create_with_retry(
                feishu['app_token'], feishu['table_id'], token, records
            )
            success_count += batch_success
        except Exception as e:
            failed_batches.append({'batch': i + 1, 'error': str(e)})
        
        time.sleep(0.5)
    
    # æ›´æ–°ç¼“å­˜ï¼ˆå¦‚æœæœ‰æˆåŠŸåŒæ­¥çš„è®°å½•ï¼‰
    if success_count > 0:
        # å°†æ–°åŒæ­¥æˆåŠŸçš„å•†å“IDæ·»åŠ åˆ°ç¼“å­˜ä¸­
        successfully_synced_ids = [p['productId'] for p in new_products[:success_count]]
        updated_ids = existing_ids.union(set(successfully_synced_ids))
        save_feishu_id_cache(cache_file, updated_ids)
    
    # ä¿å­˜è¯¦ç»†åŒæ­¥æ—¥å¿—
    log_dir = Path(__file__).parent.parent / 'sync_logs'
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S-%fZ')[:-3] + 'Z'
    log_file = log_dir / f'sync_result_{timestamp}.json'
    
    log_data = {
        'syncedAt': datetime.now().isoformat(),
        'inputFile': str(dedup_file),
        'totalLocal': len(all_products),
        'existingInFeishu': len(existing_ids),
        'newProductsFound': len(new_products),
        'successCount': success_count,
        'failedBatches': failed_batches,
        'totalBatches': total_batches,
        'batchSize': batch_size
    }
    
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)
    
    # æœ€å°åŒ–æ§åˆ¶å°è¾“å‡º
    print(f"âœ“ åŒæ­¥å®Œæˆ: {success_count}/{len(new_products)} æ¡")
    print(f"âœ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return log_file

if __name__ == '__main__':
    main()
